{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [1, 3], [1, 4], [1, 5], [2, 3], [2, 4], [2, 5], [3, 4], [3, 5], [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def combine(l, n): \n",
    "    answers = []\n",
    "    one = [0] * n \n",
    "    def next_c(li = 0, ni = 0): \n",
    "        if ni == n:\n",
    "            answers.append(copy.copy(one))\n",
    "            return\n",
    "        for lj in range(li, len(l)):\n",
    "            one[ni] = l[lj]\n",
    "            next_c(lj + 1, ni + 1)\n",
    "    next_c()\n",
    "    return answers\n",
    "\n",
    "print(combine([1, 2, 3, 4, 5], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Site', 3, 7)\n"
     ]
    }
   ],
   "source": [
    "def printLCSSubStr(X: str, Y: str,\n",
    "                 m: int, n: int):\n",
    "\n",
    "\n",
    "    LCSuff = [[0 for i in range(n + 1)]\n",
    "                for j in range(m + 1)]\n",
    "\n",
    "    length = 0\n",
    "    row, col = 0, 0\n",
    "\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                LCSuff[i][j] = 0\n",
    "            elif X[i - 1] == Y[j - 1]:\n",
    "                LCSuff[i][j] = LCSuff[i - 1][j - 1] + 1\n",
    "                if length < LCSuff[i][j]:\n",
    "                    length = LCSuff[i][j]\n",
    "                    row = i\n",
    "                    col = j\n",
    "            else:\n",
    "                LCSuff[i][j] = 0\n",
    "\n",
    "\n",
    "    if length == 0:\n",
    "        return 0,0,0\n",
    "\n",
    "\n",
    "    resultStr = ['0'] * length\n",
    "\n",
    "\n",
    "    while LCSuff[row][col] != 0:\n",
    "        length -= 1\n",
    "        resultStr[length] = X[row - 1] # or Y[col-1]\n",
    "\n",
    "\n",
    "        row -= 1\n",
    "        col -= 1\n",
    "\n",
    "\n",
    "    return(''.join(resultStr), row, len(''.join(resultStr))+row)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = \"OldSite\"\n",
    "    Y = \"NewSite\"\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "\n",
    "    print(printLCSSubStr(X, Y, m, n))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_med(L):\n",
    "    l = len(L) \n",
    "    L.sort()\n",
    "    if l%2 == 0:\n",
    "        m = (L[int(l/2) - 1] + L[int(l/2)]) / 2 \n",
    "        med = \"%.1f\" % m\n",
    "        return med\n",
    "    else: \n",
    "        m = L[int((l-1)/2)] \n",
    "        med = m\n",
    "        return med "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import load\n",
    "emb = load(\"../embeddings/Portuguese.bible.txt.vec\")\n",
    "# emb[\"faltas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance\n",
    "\n",
    "def load(fn):\n",
    "    embedding = {}\n",
    "    with open(fn, newline='') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                fields = line.split(\" \")\n",
    "                wf, vec = fields[0], fields[1:]\n",
    "                embedding[wf] = np.array(vec, dtype=np.double)\n",
    "    return embedding\n",
    "\n",
    "def similarity(wf1,wf2,emb):\n",
    "    if wf1 in emb and wf2 in emb:\n",
    "        return 1 - scipy.spatial.distance.cosine(emb[wf1], emb[wf2])\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resgatar-se', 'resgatarem', 'resgatar'], ['resgatar-se', 'matar-lhes', 'arrebatar-lhes', 'tratar-vos', 'tratar-se', 'arrebatar-te-á', 'atar-lhe', 'matar-te', 'tratar-nos', 'tratar-te', 'matar-vos', 'matar-se', 'matar-vos-ei', 'matar-me', 'maltratar-se', 'tratar-me'], ['apesar', 'pesar', 'pesarem', 'pesar-me'], ['pesarem', 'cesareia'], ['guimel', 'guimezô'], ['honra', 'honrará', 'honrar', 'honrares', 'honrado', 'honrar-me', 'honrarias', 'honrando', 'honraram', 'honrarem', 'honravas', 'honram', 'honrassem', 'honra-me', 'honrados', 'honras', 'honrarei', 'honradez', 'honrarás', 'honrada'], ['desonrar', 'desonrares', 'honrarei', 'honrarias', 'honraram', 'honrar', 'honrará', 'honrarem', 'honrarás', 'honrar-me', 'honrares', 'desonraram'], ['honrará', 'honrarás'], ['miserável', 'misericórdia', 'miserefot', 'misericórdias', 'misericordiosos', 'miseráveis', 'misericordioso'], ['misericórdias', 'misericordiosos', 'misericórdia', 'misericordioso']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_word = []\n",
    "file = open(\"Portuguese_just20_remove_big.txt\",'r')\n",
    "lines = file.readlines()\n",
    "datalists = []\n",
    "indexlist_id = []\n",
    "indexlist_ref = []\n",
    "indexlist_mb = []\n",
    "indexlist_tx = []\n",
    "\n",
    "for index, line in enumerate(lines):\n",
    "    if line == '\\n':\n",
    "        indexlist_id.append(index)\n",
    "\n",
    "all_list = []\n",
    "for i in range(len(indexlist_id)):\n",
    "    word_list = []\n",
    "    if i == len(indexlist_id) - 1:\n",
    "        current_lines = lines[indexlist_id[i]:]\n",
    "    else:\n",
    "        current_lines = lines[indexlist_id[i]: indexlist_id[i + 1]]\n",
    "    for line in current_lines:\n",
    "        if line != '\\n':\n",
    "            word = line.split(\"\\n\")\n",
    "            word_list.append(word[0])\n",
    "    all_list.append(word_list)\n",
    "print(all_list[:10])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['->+-se->em', '->+-se->', '->+em->', 'resg->mata+se->s', 'resg->arre+se->-lhes', 'resg->trat+se->s', 'resg->trat+->', 'resg->arre+se->-te-á', 'resg->atar+se->', 'resg->mata+se->', 'resg->trat+se->s', 'resg->trat+se->', 'resg->mata+se->', 'resg->mata+->', 'resg->mata+se->-ei', 'resg->mata+se->', 'resg->malt+->e', 'resg->trat+se->', 'm->a+->lhes', 'm->t+lhes->-vos', 'm->t+lhes->-se', 'm->a+lhes->tar-te-á', 'm->a+s->', '->+lhes->te', 'm->t+lhes->-nos', 'm->t+lhes->-te', '->+lhes->vos', '->+lhes->se', '->+lhes->vos-ei', '->+lhes->me', 'm->m+lhes->tar-se', 'm->t+lhes->-me', 'arreb->trata+lhes->', 'arreb->trata+lhes->', '->+lhes->te-á', 'arreb->atar-+s->', 'arreb->matar+lhes->', 'arreb->trata+lhes->', 'arreb->trata+lhes->', 'arreb->matar+lhes->', 'arreb->matar+lhes->', 'arreb->matar+lhes->ei', 'arreb->matar+lhes->', 'arreb->maltr+lhes->se', 'arreb->trata+lhes->', '->+vos->se', 'tr->ar+vos->ar-te-á', 'tr->at+vos->e', 'tr->ma+vos->e', '->+vos->nos', '->+vos->te', 'tr->ma+->', 'tr->ma+vos->e', 'tr->ma+->ei', 'tr->ma+vos->e', '->+vos->ar-se', '->+vos->me', 'tr->ar+se->ar-te-á', 'tr->at+se->e', 'tr->ma+se->e', '->+se->nos', '->+se->te', 'tr->ma+se->os', 'tr->ma+->', 'tr->ma+se->os-ei', 'tr->ma+se->e', '->+->-se', '->+se->me', 'arreb->atar-+te-á->', 'arreb->matar+-á->', 'arreb->trata+te-á->', 'arreb->trata+-á->', 'arreb->matar+te-á->', 'arreb->matar+te-á->', 'arreb->matar+te-á->ei', 'arreb->matar+te-á->', 'arreb->maltr+te-á->se', 'arreb->trata+te-á->', '->+lhe->-te', '->+lhe->r-nos', '->+lhe->r-te', '->+lhe->-vos', '->+lhe->-se', '->+lhe->-vos-ei', '->+lhe->-me', '->+lhe->atar-se', '->+lhe->r-me', 'm->t+te->-nos', 'm->t+->e', '->+te->vos', '->+te->se', '->+te->vos-ei', '->+te->me', 'm->m+te->tar-se', 'm->t+te->-me', '->+nos->te', 'tr->ma+nos->os', 'tr->ma+nos->e', 'tr->ma+nos->os-ei', 'tr->ma+nos->e']\n"
     ]
    }
   ],
   "source": [
    "paradigm = []\n",
    "for n in all_list:\n",
    "    if len(n)>= 2:\n",
    "        for i in combine(n, 2):\n",
    "            str1 = i[0]\n",
    "            str2 = i[1]\n",
    "#             print(str1)\n",
    "            X = i[0]\n",
    "            Y = i[1]\n",
    "            m = len(X)\n",
    "            n = len(Y)\n",
    "            s = (printLCSSubStr(X, Y, m, n))\n",
    "            if s[0] != 0:\n",
    "                extra_str1_1 = str1[0:s[1]]\n",
    "                extra_str1_2 = str1[s[2]:]\n",
    "                extra_str1 = (extra_str1_1+extra_str1_2)\n",
    "                extra_str2_1 = str2[0:s[1]]\n",
    "                extra_str2_2 = str2[s[2]:]\n",
    "                extra_str2 = extra_str2_1+extra_str2_2\n",
    "#                 paradigm.append(extra_str1 +\"->\"+extra_str2)\n",
    "                paradigm.append(extra_str1_1 +\"->\"+extra_str2_1+\"+\"+extra_str1_2 +\"->\"+extra_str2_2)\n",
    "#                 paradigm.append(extra_str1)\n",
    "print(paradigm[:100])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "overlap_para = collections.Counter(paradigm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104831\n"
     ]
    }
   ],
   "source": [
    "x1= overlap_para.most_common(len(overlap_para))\n",
    "# print(x1)\n",
    "print(len(overlap_para))\n",
    "x2 = [key for (key, count) in x1]\n",
    "# print(x2)\n",
    "over_lap2 = x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paradigm_list(all_list):\n",
    "    paradigm = []\n",
    "    for n in all_list:\n",
    "        if len(n)>= 2:\n",
    "            for i in combine(n, 2):\n",
    "                str1 = i[0]\n",
    "                str2 = i[1]\n",
    "                X = i[0]\n",
    "                Y = i[1]\n",
    "                m = len(X)\n",
    "                n = len(Y)\n",
    "                s = (printLCSSubStr(X, Y, m, n))\n",
    "                if s[0] != 0:\n",
    "                    extra_str1_1 = str1[0:s[1]]\n",
    "                    extra_str1_2 = str1[s[2]:]\n",
    "                    extra_str1 = (extra_str1_1+extra_str1_2)\n",
    "                    extra_str2_1 = str2[0:s[1]]\n",
    "                    extra_str2_2 = str2[s[2]:]\n",
    "                    extra_str2 = extra_str2_1+extra_str2_2\n",
    "                    paradigm.append(extra_str1_1 +\"->\"+extra_str2_1+\"+\"+extra_str1_2 +\"->\"+extra_str2_2)\n",
    "    return paradigm\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def paradigm_freq(generate_one):\n",
    "    x = collections.Counter(paradigm)\n",
    "    y = x.get(generate_one)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_degree_num(input_list):\n",
    "    degree_list = []\n",
    "    no_degree_list = []\n",
    "    weight_freq = []\n",
    "    list_all_freq = []\n",
    "    if len(input_list)>= 2:\n",
    "        for i in combine(input_list, 2):\n",
    "            str1 = i[0]\n",
    "            str2 = i[1]\n",
    "#             print(str1)\n",
    "            X = i[0]\n",
    "            Y = i[1]\n",
    "            m = len(X)\n",
    "            n = len(Y)\n",
    "            s = (printLCSSubStr(X, Y, m, n))\n",
    "            if s[0] != 0:\n",
    "                extra_str1_1 = str1[0:s[1]]\n",
    "                extra_str1_2 = str1[s[2]:]\n",
    "                extra_str1 = (extra_str1_1+extra_str1_2)\n",
    "                extra_str2_1 = str2[0:s[1]]\n",
    "                extra_str2_2 = str2[s[2]:]\n",
    "                extra_str2 = extra_str2_1+extra_str2_2\n",
    "                generate_one = extra_str1_1 +\"->\"+extra_str2_1+\"+\"+extra_str1_2 +\"->\"+extra_str2_2\n",
    "#                 paradigm.append(extra_str1)\n",
    "                if generate_one in paradigm:\n",
    "#                     print(str1,str2,generate_one,paradigm_freq.get(generate_one))#print out str1,2 and rules. and find rules frequency\n",
    "#                     weight_freq.append([str1,str2,generate_one,paradigm_freq(generate_one)]) \n",
    "#                     print(weight_freq)\n",
    "                    degree_list.append([str1,str2,paradigm_freq(generate_one)]) \n",
    "#                     degree_list.append(str2)\n",
    "                else:\n",
    "#                     print(str1,str2,generate_one,0)\n",
    "                    no_degree_list.append(str1)\n",
    "                    no_degree_list.append(str2)\n",
    "#     x = collections.Counter(degree_list)\n",
    "    x = degree_list\n",
    "    Weight_freq = {}\n",
    "    for xxx in input_list:\n",
    "        weight_num = 0\n",
    "        degree_num = 0\n",
    "        for yyy in x:\n",
    "            if xxx in yyy:\n",
    "                weight_num = yyy[2] +weight_num\n",
    "                degree_num = degree_num+1\n",
    "        Weight_freq[xxx]= [weight_num, degree_num]\n",
    "    return Weight_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualify_candidate(paradigm_group,Weight_freq_list):\n",
    "    candidate_words = []\n",
    "    all_words_each_group = []\n",
    "    for words2 in paradigm_group:\n",
    "        each_weight_num = Weight_freq_list.get(words2)[0]\n",
    "        all_words_each_group.append(each_weight_num)\n",
    "    median = Find_med(all_words_each_group)\n",
    "    for words2 in paradigm_group:\n",
    "        each_degree_num = Weight_freq_list.get(words2)[1]\n",
    "        each_weight_num = Weight_freq_list.get(words2)[0]\n",
    "        if each_degree_num != None and each_degree_num > len(paradigm_group)/2 and each_weight_num > float(median):\n",
    "            candidate_words.append(words2)\n",
    "            if len(candidate_words) >=2:\n",
    "                break\n",
    "            \n",
    "    return candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_weight(listwords2,Weight_freq_list):\n",
    "    all_words_each_group = []\n",
    "    for words2 in listwords2:\n",
    "        each_weight_num = Weight_freq_list.get(words2)[0]\n",
    "        all_words_each_group.append(each_weight_num)\n",
    "#         print(weight)\n",
    "#         each_degree_num = degree_num_list.get(words2)\n",
    "#         print(words2 +' '+str(each_degree_num)+' '+str(Find_med(all_words_each_group))+ ' '+str(len(listwords)))\n",
    "#         if each_degree_num != None and each_degree_num > len(listwords)/2 and weight > float(Find_med(all_words_each_group)):\n",
    "#             candidate_words.append(words2)\n",
    "    return all_words_each_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_of_the_word1 = []\n",
    "file88 = open(\"Portuguese.5.preds\",'r')\n",
    "lines88 = file88.readlines()\n",
    "for line in lines88:\n",
    "    if line != '\\n':\n",
    "        word = line.strip()\n",
    "        all_of_the_word1.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_of_the_word = []\n",
    "for m in all_of_the_word1:\n",
    "    if m not in all_of_the_word:\n",
    "        all_of_the_word.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103496\n",
      "28365\n",
      "28365\n"
     ]
    }
   ],
   "source": [
    "print(len(all_of_the_word1))\n",
    "print(len(set(all_of_the_word1)))\n",
    "print(len(all_of_the_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Portuguese_test_predict.txt','w') as file50:\n",
    "    while len(all_of_the_word) != 0:\n",
    "        for n in all_of_the_word:\n",
    "            small_group = []\n",
    "            small_group.append(n)\n",
    "            already_processed = []\n",
    "            for n1 in small_group:\n",
    "                if n1 not in already_processed:\n",
    "                    already_processed.append(n)\n",
    "                    for m in over_lap2:\n",
    "                        prefix = m.split(\"+\")[0]\n",
    "                        suffix = m.split(\"+\")[1]\n",
    "                        prefix_left = prefix.split(\"->\")[0]\n",
    "                        prefix_right = prefix.split(\"->\")[1]\n",
    "                        suffix_left = suffix.split(\"->\")[0]\n",
    "                        suffix_right = suffix.split(\"->\")[1]\n",
    "                        if n1.startswith(prefix_left) and n1.endswith(suffix_left):\n",
    "                            yy = len(prefix_left)\n",
    "                            xx = n1.rindex(suffix_left)\n",
    "                            new_item = prefix_right + n1[yy:xx] + suffix_right\n",
    "                            if new_item in all_of_the_word and new_item not in small_group:\n",
    "                                small_group.append(new_item)\n",
    "#                                 print(new_item)\n",
    "                        if n1.startswith(prefix_right) and n1.endswith(suffix_right):\n",
    "                            yy = len(prefix_right)\n",
    "                            xx = n1.rindex(suffix_right)\n",
    "                            new_item = prefix_left + n1[yy:xx] + suffix_left\n",
    "                            if new_item in all_of_the_word and new_item not in small_group:\n",
    "                                small_group.append(new_item)\n",
    "#                                 print(new_item)\n",
    "            break_time = 0\n",
    "            if len(small_group)>=20 and break_time == 0:\n",
    "                break_time += 1\n",
    "                degree_num_list = Get_degree_num(small_group)\n",
    "                all_words_each_group = list_of_weight(small_group,degree_num_list)\n",
    "                candidate_words = qualify_candidate(small_group,degree_num_list)\n",
    "                median_num = Find_med(all_words_each_group)\n",
    "#                 print(median_num)\n",
    "                if len(candidate_words) >=2:\n",
    "                    for each_word in small_group:\n",
    "                        each_word_weight = degree_num_list.get(each_word)[0]\n",
    "                        each_word_degree_num = degree_num_list.get(each_word)[1]\n",
    "                        if each_word_degree_num != None and each_word_degree_num <=len(small_group)/3 and each_word_weight <= float(median_num):\n",
    "                            cosine_sim1 = similarity(each_word,candidate_words[0],emb)\n",
    "                            cosine_sim2 = similarity(candidate_words[0],candidate_words[1],emb)\n",
    "                            if abs(cosine_sim1) >= 0.5 and abs(cosine_sim2 - cosine_sim1) <= 0.3:\n",
    "                                file50.write(each_word + '\\n')\n",
    "                                all_of_the_word.remove(each_word)\n",
    "                        else:\n",
    "                            if each_word in all_of_the_word:\n",
    "                                file50.write(each_word + '\\n')\n",
    "                                all_of_the_word.remove(each_word)\n",
    "\n",
    "                else:\n",
    "                    for each_word in small_group:  \n",
    "                        if each_word in all_of_the_word:\n",
    "                            file50.write(each_word + '\\n')  \n",
    "                            all_of_the_word.remove(each_word)      \n",
    "                file50.write('\\n')\n",
    "            if len(small_group)<20 and break_time == 0:\n",
    "#                 print(small_group)\n",
    "                for each_word in small_group:  \n",
    "                    if each_word in all_of_the_word:\n",
    "                        all_of_the_word.remove(each_word)\n",
    "                        file50.write(each_word + '\\n') \n",
    "                file50.write('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_of_the_word1 = []\n",
    "file88 = open(\"Portuguese_test_predict.txt\",'r')\n",
    "lines88 = file88.readlines()\n",
    "for line in lines88:\n",
    "    if line != '\\n':\n",
    "        word = line.strip()\n",
    "        all_of_the_word1.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "print(len(all_of_the_word1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matar-se': [728, 2], 'matar': [773, 2], 'matar-te': [379, 2]}\n",
      "['matar']\n"
     ]
    }
   ],
   "source": [
    "test = ['matar-se', 'matar', 'matar-te']\n",
    "test1 = ['pesarem', 'pesado', 'pesados', 'pesar-me', 'pesavam', 'pesadas', 'pesou-se', 'casarem', 'cesareia', 'casares', 'pisarem', 'tocarem']\n",
    "test2 = ['honestas', 'honestos', 'honestidade', 'honrará', 'adorará', 'livrará', 'entrará', 'honrar', 'adorar', 'entrariam', 'entraria', 'adorando', 'entrando', 'dobrando']\n",
    "\n",
    "# a = qualify_candidate(test)\n",
    "# print(a)\n",
    "# b = list_of_weight(test)\n",
    "c = Get_degree_num(test)\n",
    "print(c)\n",
    "\n",
    "a = qualify_candidate(test,c)\n",
    "print(a)\n",
    "# d = 'matar-se'\n",
    "# num = 0\n",
    "# for x in c:\n",
    "#     if d in x:\n",
    "#         num = num+1\n",
    "# print(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ['matar-se', 'matar', 'matar-te', 'matar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matar-se': [728, 2], 'matar': [773, 2], 'matar-te': [379, 2]}\n"
     ]
    }
   ],
   "source": [
    "x = c\n",
    "Weight_freq = {}\n",
    "for xxx in n:\n",
    "    weight_num = 0\n",
    "    degree_num = 0\n",
    "    for yyy in x:\n",
    "        if xxx in yyy:\n",
    "            weight_num = yyy[3] +weight_num\n",
    "            degree_num = degree_num+1\n",
    "    Weight_freq[xxx]= [weight_num, degree_num]\n",
    "print(Weight_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773\n"
     ]
    }
   ],
   "source": [
    "yc = Weight_freq.get('matar')[0]\n",
    "print(yc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
